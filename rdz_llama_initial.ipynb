{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5a67c2",
   "metadata": {},
   "source": [
    "# Import bilingual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b0b47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Data Sample: ['Many of the messages are for you and the world.\"', '“Who make another god along with Allah; but they will come to know.”', '111:1 The power of Abu Lahab will perish, and he will perish.', 'You can fail with pride.\"\"', 'Q4.151: These it is that are truly unbelievers, and We have prepared for the unbelievers a disgraceful chastisement.']\n",
      "Korean Data Sample: ['많은 메시지는 너와 세상을 위한 것이다.\"', '하나님과 함께 다른 신을 두는 자들은 곧 깨닫게 될 것이니라.', '111:1 아불라하브(Abu Lahab)의 손이 멸망하고 그가 파멸할 것이며', '실패해도 자부심 가져라(You can fail with pride).\"', '151실로 이들이야말로 불신자들이거늘 하나님은 이 불신자들에게 치욕스러운 벌을 준비하시었노라']\n",
      "Scores Data Sample: ['1.2491112', '1.2481863', '1.2453892', '1.243128', '1.2424322']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "#import pandas as pd\n",
    "\n",
    "def load_ccmatrix(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # English, Korean, and Score files\n",
    "        with zip_ref.open('CCMatrix.en-ko.en') as en_file:\n",
    "            en_data = en_file.read().decode('utf-8').splitlines()\n",
    "        with zip_ref.open('CCMatrix.en-ko.ko') as ko_file:\n",
    "            ko_data = ko_file.read().decode('utf-8').splitlines()\n",
    "        with zip_ref.open('CCMatrix.en-ko.scores') as scores_file:\n",
    "            scores_data = scores_file.read().decode('utf-8').splitlines()\n",
    "    \n",
    "    return en_data, ko_data, scores_data\n",
    "\n",
    "# Set path and run\n",
    "zip_path = 'en-ko.txt.zip'\n",
    "en_data, ko_data, scores_data = load_ccmatrix(zip_path)\n",
    "\n",
    "print(f\"English Data Sample: {en_data[:5]}\")\n",
    "print(f\"Korean Data Sample: {ko_data[:5]}\")\n",
    "print(f\"Scores Data Sample: {scores_data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c9e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19360332 19359322\n"
     ]
    }
   ],
   "source": [
    "print(len(en_data), len(ko_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5317c6",
   "metadata": {},
   "source": [
    "# Load llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'YOUR_HF_ACCESS_TOKEN' with your actual Hugging Face access token\n",
    "login('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')\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=1000,\n",
    "    eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0980f21",
   "metadata": {},
   "source": [
    "## baseline llama model\n",
    "\n",
    "Uses framing to modify inputs for translation task. Overall, a text generator, not fine tuned for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25604bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Translate this to korean: Hello\"\n",
    "generated = pipeline(prompt, max_length=25, num_return_sequences=1, truncation=True)\n",
    "translation = generated[0]['generated_text']\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20344472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
